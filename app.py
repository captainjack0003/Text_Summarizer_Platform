import langchain
import vertexai
from google.cloud import aiplatform
from langchain.vectorstores import Chroma

import time
from typing import Any, Mapping, List, Dict, Optional
from dataclasses import dataclass, field
from pydantic import BaseModel, Extra, root_validator
from langchain.llms.base import LLM
from langchain.embeddings.base import Embeddings
from langchain.llms.utils import enforce_stop_tokens


def rate_limit(max_per_minute):
  period = 60 / max_per_minute
  print('Waiting')
  while True:
    before = time.time()
    yield
    after = time.time()
    elapsed = after - before
    sleep_time = max(0, period - elapsed)
    if sleep_time > 0:
      print('.', end='')
      time.sleep(sleep_time)


class _VertexCommon(BaseModel):
    """Wrapper around Vertex AI large language models.

    To use, you should have the
    ``google.cloud.aiplatform.private_preview.language_models`` python package
    installed.
    """
    client: Any = None #: :meta private:
    model_name: str = "text-bison@001"
    """Model name to use."""

    temperature: float = 0.2
    """What sampling temperature to use."""

    top_p: int = 0.8
    """Total probability mass of tokens to consider at each step."""

    top_k: int = 40
    """The number of highest probability tokens to keep for top-k filtering."""

    max_output_tokens: int = 200
    """The maximum number of tokens to generate in the completion."""

    @property
    def _default_params(self) -> Mapping[str, Any]:
        """Get the default parameters for calling Vertex AI API."""
        return {
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "max_output_tokens": self.max_output_tokens
        }

    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:
        res = self.client.predict(prompt, **self._default_params)
        return self._enforce_stop_words(res.text, stop)

    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:
        if stop:
            return enforce_stop_tokens(text, stop)
        return text

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "vertex_ai"

class VertexLLM(_VertexCommon, LLM):
    model_name: str = "text-bison@001"

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that the python package exists in environment."""
        try:
            from vertexai.preview.language_models import TextGenerationModel
        except ImportError:
            raise ValueError(
                "Could not import Vertex AI LLM python package. "
            )

        try:
            values["client"] = TextGenerationModel.from_pretrained(values["model_name"])
        except AttributeError:
            raise ValueError(
                "Could not set Vertex Text Model client."
            )

        return values

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """Call out to Vertex AI's create endpoint.

        Args:
            prompt: The prompt to pass into the model.

        Returns:
            The string generated by the model.
        """
        return self._predict(prompt, stop)

from langchain.embeddings import VertexAIEmbeddings


class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):
    requests_per_minute: int
    num_instances_per_batch: int

    # Overriding embed_documents method
    def embed_documents(self, texts: List[str]):
        limiter = rate_limit(self.requests_per_minute)
        results = []
        docs = list(texts)

        while docs:
            # Working in batches because the API accepts maximum 5
            # documents per request to get embeddings
            head, docs = (
                docs[: self.num_instances_per_batch],
                docs[self.num_instances_per_batch :],
            )
            chunk = self.client.get_embeddings(head)
            results.extend(chunk)
            next(limiter)

        return [r.values for r in results]
    

#from document_loaders.multi_pdf_loader import MultiPDFLoader
#from langchain.document_loaders import MultiPDFLoader

# List of file names to load
#file_names = ["Ford OS Behaviors and Anchors.pdf", "8 Wastes Defn and Examples.pdf", "2023 World Mental Health Day.pdf"]

# Create a MultiPDFLoader instance with the list of file names
#loader = MultiPDFLoader(file_names)

### for pdf working code
from langchain.document_loaders import PyPDFLoader

file_names = ["Ford OS Behaviors and Anchors.pdf", "8 Wastes Defn and Examples.pdf", "2023 World Mental Health Day.pdf"]

documents = []

for file_name in file_names:
    loader = PyPDFLoader(file_name)
    documents.extend(loader.load())


####for excel
#from langchain.document_loaders.csv_loader import CSVLoader

#file_paths = ["TBEA Chat Bot Input Data.csv", "Test.csv"]
#delimiter = ","
#csv_args = {"delimiter": delimiter}
#encoding = "utf-8"

#documents = []

#for file_path in file_paths:
    #loader = CSVLoader(file_path, csv_args=csv_args, encoding=encoding)
    #documents.extend(loader.load())

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
# Create a RecursiveCharacterTextSplitter instance


# Import the required modules
import streamlit as st

# Initialize the language model
llm_text_bison001 = VertexLLM(
    model_name='text-bison@001',
    max_output_tokens=256,
    temperature=0,
    top_p=0.8,
    top_k=40,
    verbose=True,
)

# Embedding
EMBEDDING_QPM = 100
EMBEDDING_NUM_BATCH = 5
embeddings = CustomVertexAIEmbeddings(
    requests_per_minute=EMBEDDING_QPM,
    num_instances_per_batch=EMBEDDING_NUM_BATCH,
)

db = Chroma.from_documents(docs, embeddings)

# Define a function for generating an answer to a question
def chat_TBEA(User_Input):
    
    # Retrieve relevant content from the document store
    contexts_ss = db.similarity_search(User_Input, k=3)
    relavent_content_ss = ""
    for context_ss in contexts_ss:
        relavent_content_ss += context_ss.page_content


    # Define a template for generating a prompt
    from langchain.prompts import PromptTemplate
    template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use five sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer. 
    {relavent_content_ss}
    Question: {question}"""

    # Create a prompt template using the defined template
    prompt_template = PromptTemplate(
        input_variables=["relavent_content_ss", "question"],
        template=template
    )

    # Create a retrieval-based QA chain using the language model and the document store
    from langchain.chains import RetrievalQA
    qa_chain = RetrievalQA.from_chain_type(
        llm_text_bison001,
        retriever=db.as_retriever(),
        return_source_documents=True,
        chain_type='stuff',
    )

    # Generate an answer using the QA chain and the user input
    result = qa_chain({"query": User_Input})
    answer = result["result"]

    # Display the answer in a container
    with st.container():
        st.write(answer)

# Define the Streamlit app
st.title('ðŸ¦œðŸ”— Trial App')

# Create a form for uploading files and entering text
#with st.form('my_form'):
    #uploaded_files = st.file_uploader('Upload your files:', type='pdf', accept_multiple_files=True)
    #text = st.text_area('Enter text:', '')
    #submitted = st.form_submit_button('Submit')

# Create a form for entering text
with st.form('my_form'):
    text = st.text_area('Enter text:', '')
    submitted = st.form_submit_button('Submit')

    # Run the chat_TBEA function when the form is submitted
    if submitted:
        chat_TBEA(text)


